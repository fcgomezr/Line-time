# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
<p>In this project we have used UCI Bank Marketing dataset, which is related with direct marketing campaigns of a Portuguese baking institution. The classification goal is predict if the client will subscribe a term deposit (variable 'y'). <a href="https://archive.ics.uci.edu/ml/datasets/Bank+Marketing"> Read More </a></p>

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
<p>In this project, we have used scikit-learn Logistic Regression and tuned the hyperparameters(optimal) using HyperDrive. We also used AutoML to build and optimize a model on the same dataset, so that we can compare the results of the two methods.
The best performing model was obtained through AutoML - <strong> VotingEnsemble </strong> with accuracy of <b>0.9154</b></p>

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
<ol>
  <li>Setup Training Script
    <ul>
      <li> Import data using TabularDatasetFactory </li>
      <li> Cleaning of data </li>
      <li> Splitting of data into train and test data </li>
      <li> Using scikit-learn logistic regression model for classification </li>
    </ul>
  </li><br>
  <li>Create SKLearn Estimator for training the model selected (logistic regression) by passing the training script and later the estimator is passed to the hyperdrive                 configuration</li><br>
  <li> Configuration of Hyperdrive
    <ul>
      <li> Selection of parameter sampler </li>
      <li> Selection of primary metric </li>
      <li> Selection of early termination policy </li>
      <li> Selection of estimator (SKLearn) </li>
      <li> Allocation of resources </li>
      <li> Other configuration details </li>
    </ul>
  </li><br>  
  <li>Save the trained optimized model</li>
</ol>

**What are the benefits of the parameter sampler you chose?**
<p>The parameter sampler I chose was <i>RandomParameterSampling</i> because it supports both discrete and continuous hyperparameters. It supports early termination of low-performance runs and supports early stopping policies. In random sampling , the hyperparameter () values are randomly selected from the defined search space. </p>
**What are the benefits of the early stopping policy you chose?**
<p> The early stopping policy I chose was <i>BanditPolicy</i> because it is based on slack factor and evaluation interval. Bandit terminates runs where the primary etric is not within the specified slack factor compared to the best performing run</p>
## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
